{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8ab65a7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip install transformers deepspeed-mii --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3da0834c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Nov 17 06:24:48 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 510.47.03    Driver Version: 510.47.03    CUDA Version: 11.6     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla T4            On   | 00000000:00:1E.0 Off |                    0 |\n",
      "| N/A   29C    P8    14W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ebbc66bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fdaa59c1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022-11-17 06:25:07,051] [INFO] [deployment.py:87:deploy] ************* MII is using DeepSpeed Optimizations to accelerate your model *************\n",
      "[2022-11-17 06:25:07,235] [INFO] [server_client.py:219:_initialize_service] MII using multi-gpu deepspeed launcher:\n",
      " ------------------------------------------------------------\n",
      " task-name .................... text-generation \n",
      " model ........................ bigscience/bloom-3b \n",
      " model-path ................... /tmp/mii_models \n",
      " port ......................... 50050 \n",
      " provider ..................... hugging-face \n",
      " ------------------------------------------------------------\n",
      "[2022-11-17 06:25:08,442] [WARNING] [runner.py:179:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.\n",
      "[2022-11-17 06:25:08,486] [INFO] [runner.py:508:main] cmd = /home/ec2-user/anaconda3/envs/pytorch_p38/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMF19 --master_addr=127.0.0.1 --master_port=29500 --no_python --no_local_rank /home/ec2-user/anaconda3/envs/pytorch_p38/bin/python -m mii.launch.multi_gpu_server --task-name text-generation --model bigscience/bloom-3b --model-path /tmp/mii_models --port 50050 --ds-optimize --provider hugging-face --config eyJ0ZW5zb3JfcGFyYWxsZWwiOiAxLCAicG9ydF9udW1iZXIiOiA1MDA1MCwgImR0eXBlIjogImZwMzIiLCAiZW5hYmxlX2N1ZGFfZ3JhcGgiOiBmYWxzZSwgImNoZWNrcG9pbnRfZGljdCI6IG51bGwsICJkZXBsb3lfcmFuayI6IFswXSwgInRvcmNoX2Rpc3RfcG9ydCI6IDI5NTAwLCAiaGZfYXV0aF90b2tlbiI6IG51bGwsICJyZXBsYWNlX3dpdGhfa2VybmVsX2luamVjdCI6IHRydWUsICJwcm9maWxlX21vZGVsX3RpbWUiOiBmYWxzZSwgInNraXBfbW9kZWxfY2hlY2siOiBmYWxzZX0=\n",
      "[2022-11-17 06:25:09,669] [INFO] [launch.py:142:main] WORLD INFO DICT: {'localhost': [0]}\n",
      "[2022-11-17 06:25:09,669] [INFO] [launch.py:148:main] nnodes=1, num_local_procs=1, node_rank=0\n",
      "[2022-11-17 06:25:09,669] [INFO] [launch.py:161:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})\n",
      "[2022-11-17 06:25:09,669] [INFO] [launch.py:162:main] dist_world_size=1\n",
      "[2022-11-17 06:25:09,669] [INFO] [launch.py:164:main] Setting CUDA_VISIBLE_DEVICES=0\n",
      "[2022-11-17 06:25:12,256] [INFO] [server_client.py:117:_wait_until_server_is_live] waiting for server to start...\n",
      "[2022-11-17 06:25:17,261] [INFO] [server_client.py:117:_wait_until_server_is_live] waiting for server to start...\n",
      "[2022-11-17 06:25:22,266] [INFO] [server_client.py:117:_wait_until_server_is_live] waiting for server to start...\n",
      "[2022-11-17 06:25:27,270] [INFO] [server_client.py:117:_wait_until_server_is_live] waiting for server to start...\n",
      "[2022-11-17 06:25:32,275] [INFO] [server_client.py:117:_wait_until_server_is_live] waiting for server to start...\n",
      "[2022-11-17 06:25:37,278] [INFO] [server_client.py:117:_wait_until_server_is_live] waiting for server to start...\n",
      "[2022-11-17 06:25:42,283] [INFO] [server_client.py:117:_wait_until_server_is_live] waiting for server to start...\n",
      "[2022-11-17 06:25:47,288] [INFO] [server_client.py:117:_wait_until_server_is_live] waiting for server to start...\n",
      "> --------- MII Settings: ds_optimize=True, replace_with_kernel_inject=True, enable_cuda_graph=False \n",
      "[2022-11-17 06:25:49,877] [INFO] [logging.py:68:log_dist] [Rank -1] DeepSpeed info: version=0.7.5, git-hash=unknown, git-branch=unknown\n",
      "[2022-11-17 06:25:49,878] [INFO] [logging.py:68:log_dist] [Rank -1] quantize_bits = 8 mlp_extra_grouping = False, quantize_groups = 1\n",
      "Using /home/ec2-user/.cache/torch_extensions/py38_cu111 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/ec2-user/.cache/torch_extensions/py38_cu111/transformer_inference/build.ninja...\n",
      "Building extension module transformer_inference...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module transformer_inference...\n",
      "Time to load transformer_inference op: 0.3803677558898926 seconds\n",
      "[2022-11-17 06:25:51,050] [INFO] [logging.py:68:log_dist] [Rank -1] DeepSpeed-Inference config: {'layer_id': 0, 'hidden_size': 2560, 'intermediate_size': 10240, 'heads': 32, 'num_hidden_layers': -1, 'fp16': False, 'pre_layer_norm': True, 'local_rank': -1, 'stochastic_mode': False, 'epsilon': 1e-05, 'mp_size': 1, 'q_int8': False, 'scale_attention': True, 'triangular_masking': True, 'local_attention': False, 'window_size': 1, 'rotary_dim': -1, 'rotate_half': False, 'rotate_every_two': True, 'return_tuple': True, 'mlp_after_attn': True, 'mlp_act_func_type': <ActivationFuncType.GELU: 1>, 'specialized_mode': False, 'training_mp_size': 1, 'bigscience_bloom': True, 'max_out_tokens': 1024, 'scale_attn_by_inverse_layer_idx': False}\n",
      "[2022-11-17 06:25:52,293] [INFO] [server_client.py:117:_wait_until_server_is_live] waiting for server to start...\n",
      "[2022-11-17 06:25:57,298] [INFO] [server_client.py:117:_wait_until_server_is_live] waiting for server to start...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/runpy.py\", line 194, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/mii/launch/multi_gpu_server.py\", line 70, in <module>\n",
      "    main()\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/mii/launch/multi_gpu_server.py\", line 56, in main\n",
      "    inference_pipeline = load_models(task_name=args.task_name,\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/mii/models/load_models.py\", line 73, in load_models\n",
      "    engine = deepspeed.init_inference(getattr(inference_pipeline,\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/deepspeed/__init__.py\", line 307, in init_inference\n",
      "    engine = InferenceEngine(model,\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/deepspeed/inference/engine.py\", line 154, in __init__\n",
      "    self._apply_injection_policy(\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/deepspeed/inference/engine.py\", line 386, in _apply_injection_policy\n",
      "    replace_transformer_layer(\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/deepspeed/module_inject/replace_module.py\", line 949, in replace_transformer_layer\n",
      "    replaced_module = replace_module(model=model,\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/deepspeed/module_inject/replace_module.py\", line 1206, in replace_module\n",
      "    replaced_module, _ = _replace_module(model, policy)\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/deepspeed/module_inject/replace_module.py\", line 1233, in _replace_module\n",
      "    _, layer_id = _replace_module(child, policies, layer_id=layer_id)\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/deepspeed/module_inject/replace_module.py\", line 1233, in _replace_module\n",
      "    _, layer_id = _replace_module(child, policies, layer_id=layer_id)\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/deepspeed/module_inject/replace_module.py\", line 1223, in _replace_module\n",
      "    replaced_module = policies[child.__class__][0](child,\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/deepspeed/module_inject/replace_module.py\", line 939, in replace_fn\n",
      "    new_module = replace_with_policy(child,\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/deepspeed/module_inject/replace_module.py\", line 524, in replace_with_policy\n",
      "    new_module = transformer_inference.DeepSpeedTransformerInference(\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/deepspeed/model_implementations/transformers/ds_transformer.py\", line 64, in __init__\n",
      "    self.mlp = DeepSpeedMLP(self.config,\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/deepspeed/ops/transformer/inference/ds_mlp.py\", line 109, in __init__\n",
      "    self.inter_w = nn.Parameter(torch.empty(self.config.hidden_size,\n",
      "RuntimeError: CUDA out of memory. Tried to allocate 100.00 MiB (GPU 0; 14.56 GiB total capacity; 13.49 GiB already allocated; 82.50 MiB free; 13.49 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using /home/ec2-user/.cache/torch_extensions/py38_cu111 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module transformer_inference, skipping build step...\n",
      "Loading extension module transformer_inference...\n",
      "Time to load transformer_inference op: 0.04982352256774902 seconds\n",
      "Using /home/ec2-user/.cache/torch_extensions/py38_cu111 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module transformer_inference, skipping build step...\n",
      "Loading extension module transformer_inference...\n",
      "Time to load transformer_inference op: 0.048578500747680664 seconds\n",
      "[2022-11-17 06:26:01,730] [INFO] [launch.py:318:sigkill_handler] Killing subprocess 16449\n",
      "[2022-11-17 06:26:01,730] [ERROR] [launch.py:324:sigkill_handler] ['/home/ec2-user/anaconda3/envs/pytorch_p38/bin/python', '-m', 'mii.launch.multi_gpu_server', '--task-name', 'text-generation', '--model', 'bigscience/bloom-3b', '--model-path', '/tmp/mii_models', '--port', '50050', '--ds-optimize', '--provider', 'hugging-face', '--config', 'eyJ0ZW5zb3JfcGFyYWxsZWwiOiAxLCAicG9ydF9udW1iZXIiOiA1MDA1MCwgImR0eXBlIjogImZwMzIiLCAiZW5hYmxlX2N1ZGFfZ3JhcGgiOiBmYWxzZSwgImNoZWNrcG9pbnRfZGljdCI6IG51bGwsICJkZXBsb3lfcmFuayI6IFswXSwgInRvcmNoX2Rpc3RfcG9ydCI6IDI5NTAwLCAiaGZfYXV0aF90b2tlbiI6IG51bGwsICJyZXBsYWNlX3dpdGhfa2VybmVsX2luamVjdCI6IHRydWUsICJwcm9maWxlX21vZGVsX3RpbWUiOiBmYWxzZSwgInNraXBfbW9kZWxfY2hlY2siOiBmYWxzZX0='] exits with return code = 1\n",
      "[2022-11-17 06:26:02,303] [INFO] [server_client.py:117:_wait_until_server_is_live] waiting for server to start...\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "server crashed for some reason, unable to proceed",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_15093/1789195400.py\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmii\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmii_configs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"tensor_parallel\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"dtype\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"fp32\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m mii.deploy(task=\"text-generation\",\n\u001b[0m\u001b[1;32m      4\u001b[0m            \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"bigscience/bloom-3b\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m            \u001b[0mdeployment_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"bloom3b_deployment\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/mii/deployment.py\u001b[0m in \u001b[0;36mdeploy\u001b[0;34m(task, model, deployment_name, deployment_type, model_path, enable_deepspeed, enable_zero, ds_config, mii_config, version)\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0m_deploy_aml\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdeployment_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdeployment_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mversion\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mversion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mdeployment_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mDeploymentType\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLOCAL\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_deploy_local\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdeployment_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Unknown deployment type: {deployment_type}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/mii/deployment.py\u001b[0m in \u001b[0;36m_deploy_local\u001b[0;34m(deployment_name, model_path)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_deploy_local\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdeployment_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m     \u001b[0mmii\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_score_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdeployment_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/mii_cache/bloom3b_deployment/score.py\u001b[0m in \u001b[0;36minit\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;32mglobal\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m     model = mii.MIIServerClient(task,\n\u001b[0m\u001b[1;32m     30\u001b[0m                                 \u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m                                 \u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/mii/server_client.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, task_name, model_name, model_path, ds_optimize, ds_zero, ds_config, mii_configs, initialize_service, initialize_grpc_client, use_grpc_server)\u001b[0m\n\u001b[1;32m     90\u001b[0m                                                     mii_configs)\n\u001b[1;32m     91\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_grpc_server\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wait_until_server_is_live\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitialize_grpc_client\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_grpc_server\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/mii/server_client.py\u001b[0m in \u001b[0;36m_wait_until_server_is_live\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mprocess_alive\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_server_process_alive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mprocess_alive\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"server crashed for some reason, unable to proceed\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m             \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"waiting for server to start...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: server crashed for some reason, unable to proceed"
     ]
    }
   ],
   "source": [
    "import mii\n",
    "mii_configs = {\"tensor_parallel\": 1, \"dtype\": \"fp32\"}\n",
    "mii.deploy(task=\"text-generation\",\n",
    "           model=\"bigscience/bloom-3b\",\n",
    "           deployment_name=\"bloom3b_deployment\",\n",
    "           mii_config=mii_configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21030631",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = mii.mii_query_handle(\"bloom3b_deployment\")\n",
    "result = generator.query({\"query\": [\"DeepSpeed is\", \"Seattle is\"]}, do_sample=True, max_new_tokens=30)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "881b0b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "mii.terminate(\"bloom3b_deployment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e6b8644",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p38",
   "language": "python",
   "name": "conda_pytorch_p38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
